{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from itertools import compress\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Polygon\n",
    "from skimage.measure import find_contours\n",
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results_dir = '../data/segmentation_results/'\n",
    "image_folder = '../data/BCU_database/03_resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>model</th>\n",
       "      <th>expr</th>\n",
       "      <th>conf</th>\n",
       "      <th>bbox</th>\n",
       "      <th>labels</th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption_raw</th>\n",
       "      <th>caption_preprocessed</th>\n",
       "      <th>title_raw</th>\n",
       "      <th>title_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027</td>\n",
       "      <td>JOMU_32980_2k_324w.jpg</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.8280), tensor(0.7924), tensor(0.6899...</td>\n",
       "      <td>[[tensor(178.8594), tensor(116.5122), tensor(2...</td>\n",
       "      <td>[people, people, people, people, people, peopl...</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>Saint Nicholas Festival Market, Place de Notre...</td>\n",
       "      <td>saint nicholas festival market, place de notre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183</td>\n",
       "      <td>CAPO_02480_2k_324w.jpg</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.8242), tensor(0.5884), tensor(0.5781...</td>\n",
       "      <td>[[tensor(0.8775), tensor(131.9302), tensor(81....</td>\n",
       "      <td>[soldiers, a building, a building, a building]</td>\n",
       "      <td>516.0</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>[Mobilization at Perolles in August 1914]</td>\n",
       "      <td>[mobilization at perolles in august 1914]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>543</td>\n",
       "      <td>JATH_26232_2k_324w.jpg</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.7898), tensor(0.7305), tensor(0.6872...</td>\n",
       "      <td>[[tensor(97.0861), tensor(137.5495), tensor(14...</td>\n",
       "      <td>[soldiers, soldiers, a military vehicle, soldi...</td>\n",
       "      <td>950.0</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>Additional service for women, Barracks de la P...</td>\n",
       "      <td>additional service for women, barracks de la p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>JATH_10616_2k_324w.jpg</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.6900), tensor(0.6777), tensor(0.6676...</td>\n",
       "      <td>[[tensor(125.4207), tensor(111.6184), tensor(1...</td>\n",
       "      <td>[women, women, women, a street, women]</td>\n",
       "      <td>836.0</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>Procession on the route to the Alps during a w...</td>\n",
       "      <td>procession on the route to the alps during a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408</td>\n",
       "      <td>HAWI_01023_2k_324w.jpg</td>\n",
       "      <td>a photograph of a large tropical cyclone.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.7263)]</td>\n",
       "      <td>[[tensor(4.5861), tensor(13.1419), tensor(318....</td>\n",
       "      <td>[a large tropical cyclone]</td>\n",
       "      <td>814.0</td>\n",
       "      <td>a photograph of a large tropical cyclone.</td>\n",
       "      <td>a a large tropical cyclone.</td>\n",
       "      <td>Tornado over Lake Morat</td>\n",
       "      <td>tornado over lake morat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                filename  \\\n",
       "0  1027  JOMU_32980_2k_324w.jpg   \n",
       "1   183  CAPO_02480_2k_324w.jpg   \n",
       "2   543  JATH_26232_2k_324w.jpg   \n",
       "3   430  JATH_10616_2k_324w.jpg   \n",
       "4   408  HAWI_01023_2k_324w.jpg   \n",
       "\n",
       "                                             caption model     expr  \\\n",
       "0                people buying sweets at the market.  GLIP  caption   \n",
       "1  a group of soldiers stand in front of a building.  GLIP  caption   \n",
       "2     soldiers stand in front of a military vehicle.  GLIP  caption   \n",
       "3                       women walking down a street.  GLIP  caption   \n",
       "4          a photograph of a large tropical cyclone.  GLIP  caption   \n",
       "\n",
       "                                                conf  \\\n",
       "0  [tensor(0.8280), tensor(0.7924), tensor(0.6899...   \n",
       "1  [tensor(0.8242), tensor(0.5884), tensor(0.5781...   \n",
       "2  [tensor(0.7898), tensor(0.7305), tensor(0.6872...   \n",
       "3  [tensor(0.6900), tensor(0.6777), tensor(0.6676...   \n",
       "4                                   [tensor(0.7263)]   \n",
       "\n",
       "                                                bbox  \\\n",
       "0  [[tensor(178.8594), tensor(116.5122), tensor(2...   \n",
       "1  [[tensor(0.8775), tensor(131.9302), tensor(81....   \n",
       "2  [[tensor(97.0861), tensor(137.5495), tensor(14...   \n",
       "3  [[tensor(125.4207), tensor(111.6184), tensor(1...   \n",
       "4  [[tensor(4.5861), tensor(13.1419), tensor(318....   \n",
       "\n",
       "                                              labels  image_id  \\\n",
       "0  [people, people, people, people, people, peopl...    1488.0   \n",
       "1     [soldiers, a building, a building, a building]     516.0   \n",
       "2  [soldiers, soldiers, a military vehicle, soldi...     950.0   \n",
       "3             [women, women, women, a street, women]     836.0   \n",
       "4                         [a large tropical cyclone]     814.0   \n",
       "\n",
       "                                         caption_raw  \\\n",
       "0                people buying sweets at the market.   \n",
       "1  a group of soldiers stand in front of a building.   \n",
       "2     soldiers stand in front of a military vehicle.   \n",
       "3                       women walking down a street.   \n",
       "4          a photograph of a large tropical cyclone.   \n",
       "\n",
       "                                caption_preprocessed  \\\n",
       "0                people buying sweets at the market.   \n",
       "1  a group of soldiers stand in front of a building.   \n",
       "2     soldiers stand in front of a military vehicle.   \n",
       "3                       women walking down a street.   \n",
       "4                        a a large tropical cyclone.   \n",
       "\n",
       "                                           title_raw  \\\n",
       "0  Saint Nicholas Festival Market, Place de Notre...   \n",
       "1          [Mobilization at Perolles in August 1914]   \n",
       "2  Additional service for women, Barracks de la P...   \n",
       "3  Procession on the route to the Alps during a w...   \n",
       "4                            Tornado over Lake Morat   \n",
       "\n",
       "                                  title_preprocessed  \n",
       "0  saint nicholas festival market, place de notre...  \n",
       "1          [mobilization at perolles in august 1914]  \n",
       "2  additional service for women, barracks de la p...  \n",
       "3  procession on the route to the alps during a w...  \n",
       "4                            tornado over lake morat  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pickle.load(open('../data/dataset_for_segmentation.p', 'rb'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_dataset,\n",
    "        transforms=None,\n",
    "        transforms_mask=None,\n",
    "        images_folder=None,\n",
    "        desired_size=352\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.transforms_mask = transforms_mask\n",
    "        self.df_dataset = df_dataset\n",
    "        self.images_folder = images_folder\n",
    "        self.desired_size = desired_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df_dataset.loc[idx].copy()\n",
    "        img_path = self.images_folder + data['filename']\n",
    "        pilimg = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(pilimg)\n",
    "        mask = np.zeros((len(data['bbox']), img.shape[0], img.shape[1]))\n",
    "\n",
    "        old_size = img.shape[:2]\n",
    "        ratio = float(self.desired_size) / max(old_size)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "        original_img = img.copy()\n",
    "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = self.desired_size - new_size[1]\n",
    "        delta_h = self.desired_size - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "        )\n",
    "        new_mask = []\n",
    "        if len(data['conf']) == 0:\n",
    "            return {\n",
    "                'img': img,\n",
    "                'input': None,\n",
    "                'mask': None,\n",
    "                'data': data,\n",
    "                'scaled_img': None,\n",
    "            }\n",
    "        for i, box in enumerate(data['bbox']):\n",
    "            box[box < 0] = 0\n",
    "            box = box.int()\n",
    "            mask[i, box[1] : box[3], box[0] : box[2]] = 1\n",
    "            new_mask.append(\n",
    "                cv2.copyMakeBorder(\n",
    "                    cv2.resize(mask[i], (new_size[1], new_size[0])),\n",
    "                    top,\n",
    "                    bottom,\n",
    "                    left,\n",
    "                    right,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=color,\n",
    "                )\n",
    "            )\n",
    "        mask = np.array(new_mask)\n",
    "\n",
    "        if self.transforms_mask is not None:\n",
    "            transformed_mask = []\n",
    "            for i, mask_ in enumerate(mask):\n",
    "                transformed_mask.append(self.transforms_mask(mask_))\n",
    "            mask = torch.stack(transformed_mask)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        output = torch.zeros((len(data['bbox']), 3, img.shape[1], img.shape[2]))\n",
    "        for i, mask_ in enumerate(mask):\n",
    "            output[i] = img * mask_\n",
    "\n",
    "        return {\n",
    "            'original_shape': old_size,\n",
    "            'input': output,\n",
    "            'mask': mask,\n",
    "            'data': data,\n",
    "            'original_img': original_img,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_mask_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "dataset_test = FiftyOneTorchDataset(dataset, images_folder=image_folder, transforms=transform_test, transforms_mask = transform_mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933], [0,0,0]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.3):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def get_color(label, set_label):\n",
    "    for i, elem in enumerate(set_label):\n",
    "        if elem.lower() == label.lower():\n",
    "            return i\n",
    "    return 6\n",
    "\n",
    "from matplotlib.transforms import Affine2D, offset_copy\n",
    "\n",
    "\n",
    "def rainbow_text(x, y, strings, colors, orientation='horizontal',\n",
    "                 ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Take a list of *strings* and *colors* and place them next to each\n",
    "    other, with text strings[i] being shown in colors[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : float\n",
    "        Text position in data coordinates.\n",
    "    strings : list of str\n",
    "        The strings to draw.\n",
    "    colors : list of color\n",
    "        The colors to use.\n",
    "    orientation : {'horizontal', 'vertical'}\n",
    "    ax : Axes, optional\n",
    "        The Axes to draw into. If None, the current axes will be used.\n",
    "    **kwargs\n",
    "        All other keyword arguments are passed to plt.text(), so you can\n",
    "        set the font size, family, etc.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    t = ax.transData\n",
    "    fig = ax.figure\n",
    "    canvas = fig.canvas\n",
    "\n",
    "    assert orientation in ['horizontal', 'vertical']\n",
    "    if orientation == 'vertical':\n",
    "        kwargs.update(rotation=90, verticalalignment='bottom')\n",
    "\n",
    "    return [strings, colors]\n",
    "    # for s, c in zip(strings, colors):\n",
    "    #     text = ax.text(x, y, s + \" \", color=c, transform=t, **kwargs)\n",
    "\n",
    "    #     # Need to draw to update the text position.\n",
    "    #     text.draw(canvas.get_renderer())\n",
    "    #     ex = text.get_window_extent()\n",
    "    #     # Convert window extent from pixels to inches\n",
    "    #     # to avoid issues displaying at different dpi\n",
    "    #     ex = fig.dpi_scale_trans.inverted().transform_bbox(ex)\n",
    "\n",
    "    #     if orientation == 'horizontal':\n",
    "    #         t = text.get_transform() + \\\n",
    "    #             offset_copy(Affine2D(), fig=fig, x=ex.width, y=0)\n",
    "    #     else:\n",
    "    #         t = text.get_transform() + \\\n",
    "    #             offset_copy(Affine2D(), fig=fig, x=0, y=ex.height)\n",
    "\n",
    "def get_title(s, set_label, ax):\n",
    "    set_label = [label.replace('( ', '(').replace(' )',')') for label in set_label]\n",
    "\n",
    "    text = [s for s in re.split('(' + ('|').join(set_label).replace('(','\\(').replace(')','\\)') + ')', s, flags=re.IGNORECASE) if s.strip()]\n",
    "    colors = []\n",
    "    for word in text:\n",
    "        colors.append(COLORS[get_color(word.replace('( ', '(').replace(' )',')'), set_label)])\n",
    "    # rainbow_text(-100, -30, text, colors, size=12, ax=ax)\n",
    "    return {'text': text, 'colors': colors}\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(ax, pil_img, results, expr, masks=None, conf=0.7):\n",
    "    if results is None:\n",
    "        return ax\n",
    "    keep = results['conf'] > conf\n",
    "    scores = results['conf'][keep]\n",
    "    boxes = results['bbox'][keep]\n",
    "    labels = list(compress(results['labels'], keep))\n",
    "    set_label = set(labels)\n",
    "    np_image = np.array(pil_img)\n",
    "    if masks is not None:\n",
    "        masks = masks[keep]\n",
    "        # for i, bbox in enumerate(boxes):\n",
    "        #     if ((bbox[2]-bbox[0]) * (bbox[3]-bbox[1])) / (np_image.shape[0] * np_image.shape[1]) > 0.7:\n",
    "        #         masks[i] = 0\n",
    "\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = []\n",
    "        for bbox in boxes:\n",
    "            masks.append(None)\n",
    "\n",
    "    labels = [x for _,x in sorted(zip(boxes[:,0],labels))]\n",
    "    scores = [x for _,x in sorted(zip(boxes[:,0],scores))]\n",
    "    masks = [x for _,x in sorted(zip(boxes[:,0],masks),key=lambda x: x[0])]\n",
    "    boxes = [x for _,x in sorted(zip(boxes[:,0],boxes),key=lambda x: x[0])]\n",
    "\n",
    "    title = get_title(expr, set_label, ax)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask in zip(scores, boxes, labels, masks):\n",
    "        c = colors[get_color(l, set_label)]\n",
    "        # ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "        #                            fill=False, color=c, linewidth=1))\n",
    "\n",
    "        if mask is None:\n",
    "            # ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "            #                        fill=False, color=c, linewidth=1))\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "        # padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        # padded_mask[1:-1, 1:-1] = mask\n",
    "        # contours = find_contours(padded_mask, 0.5)\n",
    "        # for verts in contours:\n",
    "        #     verts = np.fliplr(verts) - 1\n",
    "        #     p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "        #     ax.add_patch(p)\n",
    "\n",
    "\n",
    "    ax.imshow(np_image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, data):\n",
    "        if data['mask'] is None:\n",
    "            return data, None\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        for img_ in data['input']:\n",
    "            predictions.append(self.model(img_[None,:,:,:].cuda()).cpu().detach().numpy())\n",
    "        output = []\n",
    "        bboxes = []\n",
    "        for mask, prediction in zip(data['mask'], predictions):\n",
    "            output.append(mask * prediction[0])\n",
    "            bboxes.append(self.get_bbox_from_mask(mask[0]))\n",
    "        output = torch.concat(output)\n",
    "\n",
    "        if data['original_shape'][0] > data['original_shape'][1]:\n",
    "            resize = T.Resize(data['original_shape'][0], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[2] - data['original_shape'][1]) // 2\n",
    "            resized_output = resized_output[:,:,offset:offset+data['original_shape'][1]]\n",
    "        else:\n",
    "            resize = T.Resize(data['original_shape'][1], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[1]  - data['original_shape'][0]) // 2\n",
    "            resized_output = resized_output[:,offset:offset+data['original_shape'][0]]\n",
    "\n",
    "\n",
    "        return data, resized_output\n",
    "\n",
    "    def get_bbox_from_mask(self, mask, mask_value = 1):\n",
    "        mask[mask<0] = 0\n",
    "        if mask_value is None:\n",
    "            seg = np.where(mask != 0)\n",
    "        else:\n",
    "            seg = np.where(mask == mask_value)\n",
    "        if seg[0].size <= 0 or seg[1].size <= 0:\n",
    "            return np.zeros((4,), dtype = int)\n",
    "        min_x = np.min(seg[1])\n",
    "        min_y = np.min(seg[0])\n",
    "        max_x = np.max(seg[1])\n",
    "        max_y = np.max(seg[0])\n",
    "\n",
    "        return [min_x, min_y, max_x, max_y]\n",
    "\n",
    "\n",
    "model = Model('../model/model_segmentation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 99/1454 [00:13<03:03,  7.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3154\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3153\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3154\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39;49mndim\n\u001b[1;32m   3155\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(image_folder \u001b[39m+\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mgca()\n\u001b[0;32m---> 11\u001b[0m title \u001b[39m=\u001b[39m plot_results(ax, img\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m), data[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m], expr\u001b[39m=\u001b[39;49mresult[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mcaption\u001b[39;49m\u001b[39m'\u001b[39;49m], masks\u001b[39m=\u001b[39;49mmask, conf\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m titles[result[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m title\n\u001b[1;32m     13\u001b[0m \u001b[39m# plt.show()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 131\u001b[0m, in \u001b[0;36mplot_results\u001b[0;34m(ax, pil_img, results, expr, masks, conf)\u001b[0m\n\u001b[1;32m    121\u001b[0m     np_image \u001b[39m=\u001b[39m apply_mask(np_image, mask, c)\n\u001b[1;32m    122\u001b[0m     \u001b[39m# padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# padded_mask[1:-1, 1:-1] = mask\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39m# contours = find_contours(padded_mask, 0.5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39m#     p = Polygon(verts, facecolor=\"none\", edgecolor=c)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m#     ax.add_patch(p)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m ax\u001b[39m.\u001b[39;49mimshow(np_image)\n\u001b[1;32m    132\u001b[0m ax\u001b[39m.\u001b[39mset_xticks([])\n\u001b[1;32m    133\u001b[0m ax\u001b[39m.\u001b[39mset_yticks([])\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/matplotlib/__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5605\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5597\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5598\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5599\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5600\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5601\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5602\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   5604\u001b[0m im\u001b[39m.\u001b[39mset_data(X)\n\u001b[0;32m-> 5605\u001b[0m im\u001b[39m.\u001b[39;49mset_alpha(alpha)\n\u001b[1;32m   5606\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5607\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n\u001b[1;32m   5608\u001b[0m     im\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/matplotlib/image.py:303\u001b[0m, in \u001b[0;36m_ImageBase.set_alpha\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mSet the alpha value used for blending - not supported on all backends.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39malpha : float or 2D array-like or None\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m martist\u001b[39m.\u001b[39mArtist\u001b[39m.\u001b[39m_set_alpha_for_array(\u001b[39mself\u001b[39m, alpha)\n\u001b[0;32m--> 303\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49mndim(alpha) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39malpha must be a float, two-dimensional \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    305\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39marray, or None\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_imcache \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3156\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3154\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39mndim\n\u001b[1;32m   3155\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3156\u001b[0m     \u001b[39mreturn\u001b[39;00m asarray(a)\u001b[39m.\u001b[39mndim\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "\n",
    "titles = dict()\n",
    "\n",
    "for result in tqdm(dataset_test):\n",
    "    data, mask = model.inference(result)\n",
    "    if mask is not None:\n",
    "        img = np.array(Image.open(image_folder + data['data']['filename']).convert(\"RGB\"))\n",
    "\n",
    "        ax = plt.gca()\n",
    "        title = plot_results(ax, img.astype(int), data['data'], expr=result['data']['caption'], masks=mask, conf=0)\n",
    "        titles[result['data']['filename']] = title\n",
    "        # plt.show()\n",
    "        plt.tight_layout(pad=0)\n",
    "\n",
    "        # Save the plot to a file\n",
    "\n",
    "        plt.savefig(segmentation_results_dir + result['data']['filename'], bbox_inches=\"tight\")\n",
    "        plt.cla()\n",
    "\n",
    "import json\n",
    "\n",
    "# dump titles\n",
    "with open(segmentation_results_dir + 'titles.json', 'w') as fp:\n",
    "    json.dump(titles, fp)\n",
    "    #\n",
    "    # data['data']['mask'] = mask\n",
    "    # pickle.dump(data['data'], open(segmentation_results_dir+data['data']['filename'].replace('.jpg','.p'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(segmentation_results_dir + 'titles.json', 'w') as fp:\n",
    "    json.dump(titles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('bcu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd12328ba0a93ae9455e01a30dc1564af4ba300f8e0c01c6941d8bcfcdf1d801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
