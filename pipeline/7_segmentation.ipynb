{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from itertools import compress\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "from skimage.measure import find_contours\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results_dir = '../data/segmentation_results/'\n",
    "image_folder = '../data/BCU_database/03_resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    idx                filename  \\\n0  1027  JOMU_32980_2k_324w.jpg   \n1   183  CAPO_02480_2k_324w.jpg   \n2   543  JATH_26232_2k_324w.jpg   \n3   430  JATH_10616_2k_324w.jpg   \n4   408  HAWI_01023_2k_324w.jpg   \n\n                                             caption model     expr  \\\n0                people buying sweets at the market.  GLIP  caption   \n1  a group of soldiers stand in front of a building.  GLIP  caption   \n2     soldiers stand in front of a military vehicle.  GLIP  caption   \n3                       women walking down a street.  GLIP  caption   \n4          a photograph of a large tropical cyclone.  GLIP  caption   \n\n                                                conf  \\\n0  [tensor(0.8280), tensor(0.7924), tensor(0.6899...   \n1  [tensor(0.8242), tensor(0.5884), tensor(0.5781...   \n2  [tensor(0.7898), tensor(0.7305), tensor(0.6872...   \n3  [tensor(0.6900), tensor(0.6777), tensor(0.6676...   \n4                                   [tensor(0.7263)]   \n\n                                                bbox  \\\n0  [[tensor(178.8594), tensor(116.5122), tensor(2...   \n1  [[tensor(0.8775), tensor(131.9302), tensor(81....   \n2  [[tensor(97.0861), tensor(137.5495), tensor(14...   \n3  [[tensor(125.4207), tensor(111.6184), tensor(1...   \n4  [[tensor(4.5861), tensor(13.1419), tensor(318....   \n\n                                              labels  image_id  \\\n0  [people, people, people, people, people, peopl...    1488.0   \n1     [soldiers, a building, a building, a building]     516.0   \n2  [soldiers, soldiers, a military vehicle, soldi...     950.0   \n3             [women, women, women, a street, women]     836.0   \n4                         [a large tropical cyclone]     814.0   \n\n                                         caption_raw  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4          a photograph of a large tropical cyclone.   \n\n                                caption_preprocessed  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4                        a a large tropical cyclone.   \n\n                                           title_raw  \\\n0  Saint Nicholas Festival Market, Place de Notre...   \n1          [Mobilization at Perolles in August 1914]   \n2  Additional service for women, Barracks de la P...   \n3  Procession on the route to the Alps during a w...   \n4                            Tornado over Lake Morat   \n\n                                  title_preprocessed  \n0  saint nicholas festival market, place de notre...  \n1          [mobilization at perolles in august 1914]  \n2  additional service for women, barracks de la p...  \n3  procession on the route to the alps during a w...  \n4                            tornado over lake morat  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>filename</th>\n      <th>caption</th>\n      <th>model</th>\n      <th>expr</th>\n      <th>conf</th>\n      <th>bbox</th>\n      <th>labels</th>\n      <th>image_id</th>\n      <th>caption_raw</th>\n      <th>caption_preprocessed</th>\n      <th>title_raw</th>\n      <th>title_preprocessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1027</td>\n      <td>JOMU_32980_2k_324w.jpg</td>\n      <td>people buying sweets at the market.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8280), tensor(0.7924), tensor(0.6899...</td>\n      <td>[[tensor(178.8594), tensor(116.5122), tensor(2...</td>\n      <td>[people, people, people, people, people, peopl...</td>\n      <td>1488.0</td>\n      <td>people buying sweets at the market.</td>\n      <td>people buying sweets at the market.</td>\n      <td>Saint Nicholas Festival Market, Place de Notre...</td>\n      <td>saint nicholas festival market, place de notre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>183</td>\n      <td>CAPO_02480_2k_324w.jpg</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8242), tensor(0.5884), tensor(0.5781...</td>\n      <td>[[tensor(0.8775), tensor(131.9302), tensor(81....</td>\n      <td>[soldiers, a building, a building, a building]</td>\n      <td>516.0</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>[Mobilization at Perolles in August 1914]</td>\n      <td>[mobilization at perolles in august 1914]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>543</td>\n      <td>JATH_26232_2k_324w.jpg</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7898), tensor(0.7305), tensor(0.6872...</td>\n      <td>[[tensor(97.0861), tensor(137.5495), tensor(14...</td>\n      <td>[soldiers, soldiers, a military vehicle, soldi...</td>\n      <td>950.0</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>Additional service for women, Barracks de la P...</td>\n      <td>additional service for women, barracks de la p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>430</td>\n      <td>JATH_10616_2k_324w.jpg</td>\n      <td>women walking down a street.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.6900), tensor(0.6777), tensor(0.6676...</td>\n      <td>[[tensor(125.4207), tensor(111.6184), tensor(1...</td>\n      <td>[women, women, women, a street, women]</td>\n      <td>836.0</td>\n      <td>women walking down a street.</td>\n      <td>women walking down a street.</td>\n      <td>Procession on the route to the Alps during a w...</td>\n      <td>procession on the route to the alps during a w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>408</td>\n      <td>HAWI_01023_2k_324w.jpg</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7263)]</td>\n      <td>[[tensor(4.5861), tensor(13.1419), tensor(318....</td>\n      <td>[a large tropical cyclone]</td>\n      <td>814.0</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>a a large tropical cyclone.</td>\n      <td>Tornado over Lake Morat</td>\n      <td>tornado over lake morat</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pickle.load(open('../data/dataset_for_segmentation.p', 'rb'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_dataset,\n",
    "        transforms=None,\n",
    "        transforms_mask=None,\n",
    "        images_folder=None,\n",
    "        desired_size=352\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.transforms_mask = transforms_mask\n",
    "        self.df_dataset = df_dataset\n",
    "        self.images_folder = images_folder\n",
    "        self.desired_size = desired_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df_dataset.loc[idx].copy()\n",
    "        img_path = self.images_folder + data['filename']\n",
    "        pilimg = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(pilimg)\n",
    "        mask = np.zeros((len(data['bbox']), img.shape[0], img.shape[1]))\n",
    "\n",
    "        old_size = img.shape[:2]\n",
    "        ratio = float(self.desired_size) / max(old_size)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "        original_img = img.copy()\n",
    "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = self.desired_size - new_size[1]\n",
    "        delta_h = self.desired_size - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "        )\n",
    "        new_mask = []\n",
    "        if len(data['conf']) == 0:\n",
    "            return {\n",
    "                'img': img,\n",
    "                'input': None,\n",
    "                'mask': None,\n",
    "                'data': data,\n",
    "                'scaled_img': None,\n",
    "            }\n",
    "        for i, box in enumerate(data['bbox']):\n",
    "            box[box < 0] = 0\n",
    "            box = box.int()\n",
    "            mask[i, box[1] : box[3], box[0] : box[2]] = 1\n",
    "            new_mask.append(\n",
    "                cv2.copyMakeBorder(\n",
    "                    cv2.resize(mask[i], (new_size[1], new_size[0])),\n",
    "                    top,\n",
    "                    bottom,\n",
    "                    left,\n",
    "                    right,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=color,\n",
    "                )\n",
    "            )\n",
    "        mask = np.array(new_mask)\n",
    "\n",
    "        if self.transforms_mask is not None:\n",
    "            transformed_mask = []\n",
    "            for i, mask_ in enumerate(mask):\n",
    "                transformed_mask.append(self.transforms_mask(mask_))\n",
    "            mask = torch.stack(transformed_mask)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        output = torch.zeros((len(data['bbox']), 3, img.shape[1], img.shape[2]))\n",
    "        for i, mask_ in enumerate(mask):\n",
    "            output[i] = img * mask_\n",
    "\n",
    "        return {\n",
    "            'original_shape': old_size,\n",
    "            'input': output,\n",
    "            'mask': mask,\n",
    "            'data': data,\n",
    "            'original_img': original_img,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_mask_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "dataset_test = FiftyOneTorchDataset(dataset, images_folder=image_folder, transforms=transform_test, transforms_mask = transform_mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933], [0,0,0]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def get_color(label, set_label):\n",
    "    for i, elem in enumerate(set_label):\n",
    "        if elem.lower() == label.lower():\n",
    "            return i\n",
    "    return 6\n",
    "\n",
    "from matplotlib.transforms import Affine2D, offset_copy\n",
    "\n",
    "\n",
    "def rainbow_text(x, y, strings, colors, orientation='horizontal',\n",
    "                 ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Take a list of *strings* and *colors* and place them next to each\n",
    "    other, with text strings[i] being shown in colors[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : float\n",
    "        Text position in data coordinates.\n",
    "    strings : list of str\n",
    "        The strings to draw.\n",
    "    colors : list of color\n",
    "        The colors to use.\n",
    "    orientation : {'horizontal', 'vertical'}\n",
    "    ax : Axes, optional\n",
    "        The Axes to draw into. If None, the current axes will be used.\n",
    "    **kwargs\n",
    "        All other keyword arguments are passed to plt.text(), so you can\n",
    "        set the font size, family, etc.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    t = ax.transData\n",
    "    fig = ax.figure\n",
    "    canvas = fig.canvas\n",
    "\n",
    "    assert orientation in ['horizontal', 'vertical']\n",
    "    if orientation == 'vertical':\n",
    "        kwargs.update(rotation=90, verticalalignment='bottom')\n",
    "\n",
    "    for s, c in zip(strings, colors):\n",
    "        text = ax.text(x, y, s + \" \", color=c, transform=t, **kwargs)\n",
    "\n",
    "        # Need to draw to update the text position.\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        # Convert window extent from pixels to inches\n",
    "        # to avoid issues displaying at different dpi\n",
    "        ex = fig.dpi_scale_trans.inverted().transform_bbox(ex)\n",
    "\n",
    "        if orientation == 'horizontal':\n",
    "            t = text.get_transform() + \\\n",
    "                offset_copy(Affine2D(), fig=fig, x=ex.width, y=0)\n",
    "        else:\n",
    "            t = text.get_transform() + \\\n",
    "                offset_copy(Affine2D(), fig=fig, x=0, y=ex.height)\n",
    "\n",
    "def get_title(s, set_label, ax):\n",
    "    set_label = [label.replace('( ', '(').replace(' )',')') for label in set_label]\n",
    "\n",
    "    text = [s for s in re.split('(' + ('|').join(set_label).replace('(','\\(').replace(')','\\)') + ')', s, flags=re.IGNORECASE) if s.strip()]\n",
    "    colors = []\n",
    "    for word in text:\n",
    "        colors.append(COLORS[get_color(word.replace('( ', '(').replace(' )',')'), set_label)])\n",
    "    rainbow_text(-100, -30, text, colors, size=12, ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(ax, pil_img, results, expr, masks=None, conf=0.7):\n",
    "    if results is None:\n",
    "        return ax\n",
    "    keep = results['conf'] > conf\n",
    "    scores = results['conf'][keep]\n",
    "    boxes = results['bbox'][keep]\n",
    "    labels = list(compress(results['labels'], keep))\n",
    "    set_label = set(labels)\n",
    "    np_image = np.array(pil_img)\n",
    "    if masks is not None:\n",
    "        masks = masks[keep]\n",
    "        for i, bbox in enumerate(boxes):\n",
    "            if ((bbox[2]-bbox[0]) * (bbox[3]-bbox[1])) / (np_image.shape[0] * np_image.shape[1]) > 0.7:\n",
    "                masks[i] = 0\n",
    "\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = []\n",
    "        for bbox in boxes:\n",
    "            masks.append(None)\n",
    "\n",
    "    labels = [x for _,x in sorted(zip(boxes[:,0],labels))]\n",
    "    scores = [x for _,x in sorted(zip(boxes[:,0],scores))]\n",
    "    masks = [x for _,x in sorted(zip(boxes[:,0],masks),key=lambda x: x[0])]\n",
    "    boxes = [x for _,x in sorted(zip(boxes[:,0],boxes),key=lambda x: x[0])]\n",
    "\n",
    "    get_title(expr, set_label, ax)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask in zip(scores, boxes, labels, masks):\n",
    "        c = colors[get_color(l, set_label)]\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=1))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    ax.imshow(np_image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def inference(self, data):\n",
    "        if data['mask'] is None:\n",
    "            return data, None\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        for img_ in data['input']:\n",
    "            predictions.append(self.model(img_[None,:,:,:].cuda()).cpu().detach().numpy())\n",
    "        output = []\n",
    "        bboxes = []\n",
    "        for mask, prediction in zip(data['mask'], predictions):\n",
    "            output.append(mask * prediction[0])\n",
    "            bboxes.append(self.get_bbox_from_mask(mask[0]))\n",
    "        output = torch.concat(output)\n",
    "\n",
    "        if data['original_shape'][0] > data['original_shape'][1]:\n",
    "            resize = T.Resize(data['original_shape'][0], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[2] - data['original_shape'][1]) // 2\n",
    "            resized_output = resized_output[:,:,offset:offset+data['original_shape'][1]]\n",
    "        else:\n",
    "            resize = T.Resize(data['original_shape'][1], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[1]  - data['original_shape'][0]) // 2\n",
    "            resized_output = resized_output[:,offset:offset+data['original_shape'][0]]\n",
    "\n",
    "\n",
    "        return data, resized_output\n",
    "\n",
    "    def get_bbox_from_mask(self, mask, mask_value = 1):\n",
    "        \"\"\" Computes the 2D bounding box from the input mask\n",
    "        Args:\n",
    "            mask: The segmentation mask of the image\n",
    "            mask_value: The integer value of the object in the segmentation mask\n",
    "        Returns:\n",
    "            numpy array with shape (4,) containing the 2D bounding box\n",
    "            Boolean indicating if the object is found in the given mask or not\n",
    "        \"\"\"\n",
    "        mask[mask<0] = 0\n",
    "        if mask_value is None:\n",
    "            seg = np.where(mask != 0)\n",
    "        else:\n",
    "            seg = np.where(mask == mask_value)\n",
    "        if seg[0].size <= 0 or seg[1].size <= 0:\n",
    "            return np.zeros((4,), dtype = int)\n",
    "        min_x = np.min(seg[1])\n",
    "        min_y = np.min(seg[0])\n",
    "        max_x = np.max(seg[1])\n",
    "        max_y = np.max(seg[0])\n",
    "\n",
    "        return [min_x, min_y, max_x, max_y]\n",
    "\n",
    "\n",
    "model = Model('../model/model_segmentation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(9,5))\n",
    "\n",
    "for result in tqdm(dataset_test):\n",
    "    data, mask = model.inference(result)\n",
    "    if mask is not None:\n",
    "        img = result['original_img']\n",
    "\n",
    "        ax = plt.gca()\n",
    "        plot_results(ax, 255-(255*img), data['data'], expr=result['data']['caption'], masks=mask, conf=0)\n",
    "        plt.show()\n",
    "        # plt.savefig(segmentation_results_dir + result['data']['filename'])\n",
    "        # plt.cla()\n",
    "    #\n",
    "    # data['data']['mask'] = mask\n",
    "    # pickle.dump(data['data'], open(segmentation_results_dir+data['data']['filename'].replace('.jpg','.p'),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
