{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from itertools import compress\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Polygon\n",
    "from skimage.measure import find_contours\n",
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results_dir = '../data/segmentation_results/'\n",
    "segmentation_results_dir = '../docs/images/gallery/'\n",
    "image_folder = '../data/BCU_database/03_resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>model</th>\n",
       "      <th>expr</th>\n",
       "      <th>conf</th>\n",
       "      <th>bbox</th>\n",
       "      <th>labels</th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption_raw</th>\n",
       "      <th>caption_preprocessed</th>\n",
       "      <th>title_raw</th>\n",
       "      <th>title_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027</td>\n",
       "      <td>JOMU_32980_2k_324w.jpg</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.8280), tensor(0.7924), tensor(0.6899...</td>\n",
       "      <td>[[tensor(178.8594), tensor(116.5122), tensor(2...</td>\n",
       "      <td>[people, people, people, people, people, peopl...</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>people buying sweets at the market.</td>\n",
       "      <td>Saint Nicholas Festival Market, Place de Notre...</td>\n",
       "      <td>saint nicholas festival market, place de notre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183</td>\n",
       "      <td>CAPO_02480_2k_324w.jpg</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.8242), tensor(0.5884), tensor(0.5781...</td>\n",
       "      <td>[[tensor(0.8775), tensor(131.9302), tensor(81....</td>\n",
       "      <td>[soldiers, a building, a building, a building]</td>\n",
       "      <td>516.0</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>a group of soldiers stand in front of a building.</td>\n",
       "      <td>[Mobilization at Perolles in August 1914]</td>\n",
       "      <td>[mobilization at perolles in august 1914]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>543</td>\n",
       "      <td>JATH_26232_2k_324w.jpg</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.7898), tensor(0.7305), tensor(0.6872...</td>\n",
       "      <td>[[tensor(97.0861), tensor(137.5495), tensor(14...</td>\n",
       "      <td>[soldiers, soldiers, a military vehicle, soldi...</td>\n",
       "      <td>950.0</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>soldiers stand in front of a military vehicle.</td>\n",
       "      <td>Additional service for women, Barracks de la P...</td>\n",
       "      <td>additional service for women, barracks de la p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>JATH_10616_2k_324w.jpg</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.6900), tensor(0.6777), tensor(0.6676...</td>\n",
       "      <td>[[tensor(125.4207), tensor(111.6184), tensor(1...</td>\n",
       "      <td>[women, women, women, a street, women]</td>\n",
       "      <td>836.0</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>women walking down a street.</td>\n",
       "      <td>Procession on the route to the Alps during a w...</td>\n",
       "      <td>procession on the route to the alps during a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408</td>\n",
       "      <td>HAWI_01023_2k_324w.jpg</td>\n",
       "      <td>a photograph of a large tropical cyclone.</td>\n",
       "      <td>GLIP</td>\n",
       "      <td>caption</td>\n",
       "      <td>[tensor(0.7263)]</td>\n",
       "      <td>[[tensor(4.5861), tensor(13.1419), tensor(318....</td>\n",
       "      <td>[a large tropical cyclone]</td>\n",
       "      <td>814.0</td>\n",
       "      <td>a photograph of a large tropical cyclone.</td>\n",
       "      <td>a a large tropical cyclone.</td>\n",
       "      <td>Tornado over Lake Morat</td>\n",
       "      <td>tornado over lake morat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                filename  \\\n",
       "0  1027  JOMU_32980_2k_324w.jpg   \n",
       "1   183  CAPO_02480_2k_324w.jpg   \n",
       "2   543  JATH_26232_2k_324w.jpg   \n",
       "3   430  JATH_10616_2k_324w.jpg   \n",
       "4   408  HAWI_01023_2k_324w.jpg   \n",
       "\n",
       "                                             caption model     expr  \\\n",
       "0                people buying sweets at the market.  GLIP  caption   \n",
       "1  a group of soldiers stand in front of a building.  GLIP  caption   \n",
       "2     soldiers stand in front of a military vehicle.  GLIP  caption   \n",
       "3                       women walking down a street.  GLIP  caption   \n",
       "4          a photograph of a large tropical cyclone.  GLIP  caption   \n",
       "\n",
       "                                                conf  \\\n",
       "0  [tensor(0.8280), tensor(0.7924), tensor(0.6899...   \n",
       "1  [tensor(0.8242), tensor(0.5884), tensor(0.5781...   \n",
       "2  [tensor(0.7898), tensor(0.7305), tensor(0.6872...   \n",
       "3  [tensor(0.6900), tensor(0.6777), tensor(0.6676...   \n",
       "4                                   [tensor(0.7263)]   \n",
       "\n",
       "                                                bbox  \\\n",
       "0  [[tensor(178.8594), tensor(116.5122), tensor(2...   \n",
       "1  [[tensor(0.8775), tensor(131.9302), tensor(81....   \n",
       "2  [[tensor(97.0861), tensor(137.5495), tensor(14...   \n",
       "3  [[tensor(125.4207), tensor(111.6184), tensor(1...   \n",
       "4  [[tensor(4.5861), tensor(13.1419), tensor(318....   \n",
       "\n",
       "                                              labels  image_id  \\\n",
       "0  [people, people, people, people, people, peopl...    1488.0   \n",
       "1     [soldiers, a building, a building, a building]     516.0   \n",
       "2  [soldiers, soldiers, a military vehicle, soldi...     950.0   \n",
       "3             [women, women, women, a street, women]     836.0   \n",
       "4                         [a large tropical cyclone]     814.0   \n",
       "\n",
       "                                         caption_raw  \\\n",
       "0                people buying sweets at the market.   \n",
       "1  a group of soldiers stand in front of a building.   \n",
       "2     soldiers stand in front of a military vehicle.   \n",
       "3                       women walking down a street.   \n",
       "4          a photograph of a large tropical cyclone.   \n",
       "\n",
       "                                caption_preprocessed  \\\n",
       "0                people buying sweets at the market.   \n",
       "1  a group of soldiers stand in front of a building.   \n",
       "2     soldiers stand in front of a military vehicle.   \n",
       "3                       women walking down a street.   \n",
       "4                        a a large tropical cyclone.   \n",
       "\n",
       "                                           title_raw  \\\n",
       "0  Saint Nicholas Festival Market, Place de Notre...   \n",
       "1          [Mobilization at Perolles in August 1914]   \n",
       "2  Additional service for women, Barracks de la P...   \n",
       "3  Procession on the route to the Alps during a w...   \n",
       "4                            Tornado over Lake Morat   \n",
       "\n",
       "                                  title_preprocessed  \n",
       "0  saint nicholas festival market, place de notre...  \n",
       "1          [mobilization at perolles in august 1914]  \n",
       "2  additional service for women, barracks de la p...  \n",
       "3  procession on the route to the alps during a w...  \n",
       "4                            tornado over lake morat  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pickle.load(open('../data/dataset_for_segmentation.p', 'rb'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_dataset,\n",
    "        transforms=None,\n",
    "        transforms_mask=None,\n",
    "        images_folder=None,\n",
    "        desired_size=352\n",
    "    ):\n",
    "        self.transforms = transforms\n",
    "        self.transforms_mask = transforms_mask\n",
    "        self.df_dataset = df_dataset\n",
    "        self.images_folder = images_folder\n",
    "        self.desired_size = desired_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df_dataset.loc[idx].copy()\n",
    "        img_path = self.images_folder + data['filename']\n",
    "        pilimg = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(pilimg)\n",
    "        mask = np.zeros((len(data['bbox']), img.shape[0], img.shape[1]))\n",
    "\n",
    "        old_size = img.shape[:2]\n",
    "        ratio = float(self.desired_size) / max(old_size)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "        original_img = img.copy()\n",
    "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = self.desired_size - new_size[1]\n",
    "        delta_h = self.desired_size - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "        )\n",
    "        new_mask = []\n",
    "        if len(data['conf']) == 0:\n",
    "            return {\n",
    "                'img': img,\n",
    "                'input': None,\n",
    "                'mask': None,\n",
    "                'data': data,\n",
    "                'scaled_img': None,\n",
    "            }\n",
    "        for i, box in enumerate(data['bbox']):\n",
    "            box[box < 0] = 0\n",
    "            box = box.int()\n",
    "            mask[i, box[1] : box[3], box[0] : box[2]] = 1\n",
    "            new_mask.append(\n",
    "                cv2.copyMakeBorder(\n",
    "                    cv2.resize(mask[i], (new_size[1], new_size[0])),\n",
    "                    top,\n",
    "                    bottom,\n",
    "                    left,\n",
    "                    right,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=color,\n",
    "                )\n",
    "            )\n",
    "        mask = np.array(new_mask)\n",
    "\n",
    "        if self.transforms_mask is not None:\n",
    "            transformed_mask = []\n",
    "            for i, mask_ in enumerate(mask):\n",
    "                transformed_mask.append(self.transforms_mask(mask_))\n",
    "            mask = torch.stack(transformed_mask)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        output = torch.zeros((len(data['bbox']), 3, img.shape[1], img.shape[2]))\n",
    "        for i, mask_ in enumerate(mask):\n",
    "            output[i] = img * mask_\n",
    "\n",
    "        return {\n",
    "            'original_shape': old_size,\n",
    "            'input': output,\n",
    "            'mask': mask,\n",
    "            'data': data,\n",
    "            'original_img': original_img,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_mask_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "dataset_test = FiftyOneTorchDataset(dataset, images_folder=image_folder, transforms=transform_test, transforms_mask = transform_mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933], [0,0,0]]\n",
    "\n",
    "\n",
    "def process_word(word):\n",
    "    return word.replace('.','').replace(' ', '').replace(')','').replace('(','').replace('-','').lower()\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.3):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def get_color(label, set_label):\n",
    "    for i, elem in enumerate(set_label):\n",
    "        if process_word(elem) == process_word(label):\n",
    "            return i\n",
    "    return 6\n",
    "\n",
    "from matplotlib.transforms import Affine2D, offset_copy\n",
    "\n",
    "\n",
    "def rainbow_text(x, y, strings, colors, orientation='horizontal',\n",
    "                 ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Take a list of *strings* and *colors* and place them next to each\n",
    "    other, with text strings[i] being shown in colors[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : float\n",
    "        Text position in data coordinates.\n",
    "    strings : list of str\n",
    "        The strings to draw.\n",
    "    colors : list of color\n",
    "        The colors to use.\n",
    "    orientation : {'horizontal', 'vertical'}\n",
    "    ax : Axes, optional\n",
    "        The Axes to draw into. If None, the current axes will be used.\n",
    "    **kwargs\n",
    "        All other keyword arguments are passed to plt.text(), so you can\n",
    "        set the font size, family, etc.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    t = ax.transData\n",
    "    fig = ax.figure\n",
    "    canvas = fig.canvas\n",
    "\n",
    "    assert orientation in ['horizontal', 'vertical']\n",
    "    if orientation == 'vertical':\n",
    "        kwargs.update(rotation=90, verticalalignment='bottom')\n",
    "\n",
    "    return [strings, colors]\n",
    "    # for s, c in zip(strings, colors):\n",
    "    #     text = ax.text(x, y, s + \" \", color=c, transform=t, **kwargs)\n",
    "\n",
    "    #     # Need to draw to update the text position.\n",
    "    #     text.draw(canvas.get_renderer())\n",
    "    #     ex = text.get_window_extent()\n",
    "    #     # Convert window extent from pixels to inches\n",
    "    #     # to avoid issues displaying at different dpi\n",
    "    #     ex = fig.dpi_scale_trans.inverted().transform_bbox(ex)\n",
    "\n",
    "    #     if orientation == 'horizontal':\n",
    "    #         t = text.get_transform() + \\\n",
    "    #             offset_copy(Affine2D(), fig=fig, x=ex.width, y=0)\n",
    "    #     else:\n",
    "    #         t = text.get_transform() + \\\n",
    "    #             offset_copy(Affine2D(), fig=fig, x=0, y=ex.height)\n",
    "\n",
    "def get_title(s, set_label, ax):\n",
    "    set_label = [label.replace('( ', '(').replace(' )',')') for label in set_label]\n",
    "\n",
    "    text = [s for s in re.split('(' + ('|').join(set_label).replace('(','\\(').replace(')','\\)') + ')', s, flags=re.IGNORECASE) if s.strip()]\n",
    "    colors = []\n",
    "    for word in text:\n",
    "        colors.append(COLORS[get_color(word.replace('( ', '(').replace(' )',')'), set_label)])\n",
    "    # rainbow_text(-100, -30, text, colors, size=12, ax=ax)\n",
    "    return {'text': text, 'colors': colors}\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(ax, pil_img, results, expr, masks=None, conf=0.7):\n",
    "    if results is None:\n",
    "        return ax\n",
    "    keep = results['conf'] > conf\n",
    "    scores = results['conf'][keep]\n",
    "    boxes = results['bbox'][keep]\n",
    "    labels = list(compress(results['labels'], keep))\n",
    "    set_label = set(labels)\n",
    "    np_image = np.array(pil_img)\n",
    "    if masks is not None:\n",
    "        masks = masks[keep]\n",
    "        # for i, bbox in enumerate(boxes):\n",
    "        #     if ((bbox[2]-bbox[0]) * (bbox[3]-bbox[1])) / (np_image.shape[0] * np_image.shape[1]) > 0.7:\n",
    "        #         masks[i] = 0\n",
    "\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = []\n",
    "        for bbox in boxes:\n",
    "            masks.append(None)\n",
    "\n",
    "    labels = [x for _,x in sorted(zip(boxes[:,0],labels))]\n",
    "    scores = [x for _,x in sorted(zip(boxes[:,0],scores))]\n",
    "    masks = [x for _,x in sorted(zip(boxes[:,0],masks),key=lambda x: x[0])]\n",
    "    boxes = [x for _,x in sorted(zip(boxes[:,0],boxes),key=lambda x: x[0])]\n",
    "    set_label = [label.replace('( ', '(').replace(' )',')') for label in set_label]\n",
    "\n",
    "    text = [expr for expr in re.split('(' + ('|').join(set_label).replace('(','\\(').replace(')','\\)') + ')', expr, flags=re.IGNORECASE) if expr.strip()]\n",
    "    # title = get_title(expr, set_label, ax)\n",
    "    color = [[0,0,0] for word in text]\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask in zip(scores, boxes, labels, masks):\n",
    "        # find the color of the label\n",
    "        c = colors[get_color(l, set_label)]\n",
    "        # find where the label is in the title\n",
    "        index = [i for i, word in enumerate(text) if word.lower() == l.lower()]\n",
    "        # if there is a match, change the color of the word\n",
    "        if len(index) > 0:\n",
    "            color[index[0]] = c\n",
    "        else:\n",
    "            index = [i for i, word in enumerate(text) if process_word(word) == process_word(l)]\n",
    "            if len(index) > 0:\n",
    "                color[index[0]] = c\n",
    "            else:\n",
    "                sub_text = [word.split() for word in text]\n",
    "                sub_text = [word for sublist in sub_text for word in sublist]\n",
    "                sub_l = l.split()\n",
    "                index = []\n",
    "                for sub_l_ in sub_l:\n",
    "                    index += [i for i, word in enumerate(sub_text) if word.lower() == sub_l_.lower()]\n",
    "                # find word containing index\n",
    "                final_index = []\n",
    "                for index_ in index:\n",
    "                    final_index += ([i for i, word in enumerate(text) if word.find(sub_text[index_]) > -1])\n",
    "                if len(final_index) > 0:\n",
    "                    color[final_index[0]] = c\n",
    "                else:\n",
    "                    print('No match for', l, 'in', text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "        #                            fill=False, color=c, linewidth=1))\n",
    "\n",
    "        if mask is None:\n",
    "            # ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "            #                        fill=False, color=c, linewidth=1))\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    ax.imshow(np_image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return {'text': text, 'colors': color}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, path):\n",
    "        self.model = torch.load(path)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, data):\n",
    "        if data['mask'] is None:\n",
    "            return data, None\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        for img_ in data['input']:\n",
    "            predictions.append(self.model(img_[None,:,:,:].cuda()).cpu().detach().numpy())\n",
    "        output = []\n",
    "        bboxes = []\n",
    "        for mask, prediction in zip(data['mask'], predictions):\n",
    "            output.append(mask * prediction[0])\n",
    "            bboxes.append(self.get_bbox_from_mask(mask[0]))\n",
    "        output = torch.concat(output)\n",
    "\n",
    "        if data['original_shape'][0] > data['original_shape'][1]:\n",
    "            resize = T.Resize(data['original_shape'][0], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[2] - data['original_shape'][1]) // 2\n",
    "            resized_output = resized_output[:,:,offset:offset+data['original_shape'][1]]\n",
    "        else:\n",
    "            resize = T.Resize(data['original_shape'][1], interpolation=T.InterpolationMode.NEAREST)\n",
    "            resized_output =resize(output)\n",
    "            offset = (resized_output.shape[1]  - data['original_shape'][0]) // 2\n",
    "            resized_output = resized_output[:,offset:offset+data['original_shape'][0]]\n",
    "\n",
    "\n",
    "        return data, resized_output\n",
    "\n",
    "    def get_bbox_from_mask(self, mask, mask_value = 1):\n",
    "        mask[mask<0] = 0\n",
    "        if mask_value is None:\n",
    "            seg = np.where(mask != 0)\n",
    "        else:\n",
    "            seg = np.where(mask == mask_value)\n",
    "        if seg[0].size <= 0 or seg[1].size <= 0:\n",
    "            return np.zeros((4,), dtype = int)\n",
    "        min_x = np.min(seg[1])\n",
    "        min_y = np.min(seg[0])\n",
    "        max_x = np.max(seg[1])\n",
    "        max_y = np.max(seg[0])\n",
    "\n",
    "        return [min_x, min_y, max_x, max_y]\n",
    "\n",
    "\n",
    "model = Model('../model/model_segmentation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1051/1454 [02:44<00:57,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for a restaurant in ['actors and ', 'film director']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1176/1454 [03:02<00:48,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for a party in ['actors and film director']\n",
      "No match for a party in ['actors and film director']\n",
      "No match for a party in ['actors and film director']\n",
      "No match for a party in ['actors and film director']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1454/1454 [03:47<00:00,  6.38it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1454",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1454",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtime\u001b[39;00m \u001b[39mimport\u001b[39;00m time\n\u001b[1;32m      3\u001b[0m titles \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m i, result \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataset_test)):\n\u001b[1;32m      6\u001b[0m     data, mask \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minference(result)\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [4], line 17\u001b[0m, in \u001b[0;36mFiftyOneTorchDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 17\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf_dataset\u001b[39m.\u001b[39;49mloc[idx]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     18\u001b[0m     img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_folder \u001b[39m+\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     pilimg \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/indexing.py:1074\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1071\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1073\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1074\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/indexing.py:1313\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/indexing.py:1261\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[1;32m   1260\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1261\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/generic.py:4057\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4055\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[1;32m   4056\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4057\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4059\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   4060\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/miniconda3/envs/bcu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1454"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "from time import time\n",
    "titles = dict()\n",
    "\n",
    "for i, result in enumerate(tqdm(dataset_test)):\n",
    "    data, mask = model.inference(result)\n",
    "    if mask is not None:\n",
    "        img = np.array(Image.open(image_folder + data['data']['filename']).convert(\"RGB\"))\n",
    "\n",
    "        ax = plt.gca()\n",
    "        title = plot_results(ax, img.astype(int), data['data'], expr=result['data']['caption'], masks=mask, conf=0)\n",
    "        titles[result['data']['filename']] = title\n",
    "        # plt.show()\n",
    "        plt.tight_layout(pad=0)\n",
    "\n",
    "        # Save the plot to a file\n",
    "\n",
    "        plt.savefig(segmentation_results_dir + result['data']['filename'], bbox_inches=\"tight\")\n",
    "        plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = dict()\n",
    "\n",
    "# Iterate over the items in the JSON object\n",
    "for image, image_data in titles.items():\n",
    "    # Get the text and color data for the image\n",
    "    text = image_data[\"text\"]\n",
    "    colors = image_data[\"colors\"]\n",
    "\n",
    "    # Create an empty list to store the HTML text\n",
    "    html_text = []\n",
    "\n",
    "    # Iterate over the text and color data\n",
    "    for i in range(len(text)):\n",
    "        # Convert the RGB color values to hexadecimal\n",
    "        color = \"#{:02x}{:02x}{:02x}\".format(\n",
    "            int(colors[i][0] * 255),\n",
    "            int(colors[i][1] * 255),\n",
    "            int(colors[i][2] * 255)\n",
    "        )\n",
    "\n",
    "        # Add the text and color to the HTML text list\n",
    "        html_text.append(f'<span style=\"color: {color}\">{text[i]}</span>')\n",
    "\n",
    "    # Join the HTML text list into a single string\n",
    "    html_text = \"\".join(html_text)\n",
    "    captions[image] = html_text\n",
    "    # Print the HTML text for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save captions\n",
    "with open('../docs/_data/captions.json', 'w') as f:\n",
    "    json.dump(captions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('bcu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd12328ba0a93ae9455e01a30dc1564af4ba300f8e0c01c6941d8bcfcdf1d801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
