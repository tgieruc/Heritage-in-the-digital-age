{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import src.GLIP.maskrcnn_benchmark as maskrcnn_benchmark\n",
    "\n",
    "sys.modules['maskrcnn_benchmark'] = maskrcnn_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "    idx                filename  \\\n0  1027  JOMU_32980_2k_324w.jpg   \n1   183  CAPO_02480_2k_324w.jpg   \n2   543  JATH_26232_2k_324w.jpg   \n3   430  JATH_10616_2k_324w.jpg   \n4   408  HAWI_01023_2k_324w.jpg   \n\n                                             caption model     expr  \\\n0                people buying sweets at the market.  GLIP  caption   \n1  a group of soldiers stand in front of a building.  GLIP  caption   \n2     soldiers stand in front of a military vehicle.  GLIP  caption   \n3                       women walking down a street.  GLIP  caption   \n4          a photograph of a large tropical cyclone.  GLIP  caption   \n\n                                                conf  \\\n0  [tensor(0.8280), tensor(0.7924), tensor(0.6899...   \n1  [tensor(0.8242), tensor(0.5884), tensor(0.5781...   \n2  [tensor(0.7898), tensor(0.7305), tensor(0.6872...   \n3  [tensor(0.6900), tensor(0.6777), tensor(0.6676...   \n4                                   [tensor(0.7263)]   \n\n                                                bbox  \\\n0  [[tensor(178.8594), tensor(116.5122), tensor(2...   \n1  [[tensor(0.8775), tensor(131.9302), tensor(81....   \n2  [[tensor(97.0861), tensor(137.5495), tensor(14...   \n3  [[tensor(125.4207), tensor(111.6184), tensor(1...   \n4  [[tensor(4.5861), tensor(13.1419), tensor(318....   \n\n                                              labels  image_id  \\\n0  [people, people, people, people, people, peopl...    1488.0   \n1     [soldiers, a building, a building, a building]     516.0   \n2  [soldiers, soldiers, a military vehicle, soldi...     950.0   \n3             [women, women, women, a street, women]     836.0   \n4                         [a large tropical cyclone]     814.0   \n\n                                         caption_raw  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4          a photograph of a large tropical cyclone.   \n\n                                caption_preprocessed  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4                        a a large tropical cyclone.   \n\n                                           title_raw  \\\n0  Saint Nicholas Festival Market, Place de Notre...   \n1          [Mobilization at Perolles in August 1914]   \n2  Additional service for women, Barracks de la P...   \n3  Procession on the route to the Alps during a w...   \n4                            Tornado over Lake Morat   \n\n                                  title_preprocessed  \n0  saint nicholas festival market, place de notre...  \n1          [mobilization at perolles in august 1914]  \n2  additional service for women, barracks de la p...  \n3  procession on the route to the alps during a w...  \n4                            tornado over lake morat  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>filename</th>\n      <th>caption</th>\n      <th>model</th>\n      <th>expr</th>\n      <th>conf</th>\n      <th>bbox</th>\n      <th>labels</th>\n      <th>image_id</th>\n      <th>caption_raw</th>\n      <th>caption_preprocessed</th>\n      <th>title_raw</th>\n      <th>title_preprocessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1027</td>\n      <td>JOMU_32980_2k_324w.jpg</td>\n      <td>people buying sweets at the market.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8280), tensor(0.7924), tensor(0.6899...</td>\n      <td>[[tensor(178.8594), tensor(116.5122), tensor(2...</td>\n      <td>[people, people, people, people, people, peopl...</td>\n      <td>1488.0</td>\n      <td>people buying sweets at the market.</td>\n      <td>people buying sweets at the market.</td>\n      <td>Saint Nicholas Festival Market, Place de Notre...</td>\n      <td>saint nicholas festival market, place de notre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>183</td>\n      <td>CAPO_02480_2k_324w.jpg</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8242), tensor(0.5884), tensor(0.5781...</td>\n      <td>[[tensor(0.8775), tensor(131.9302), tensor(81....</td>\n      <td>[soldiers, a building, a building, a building]</td>\n      <td>516.0</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>[Mobilization at Perolles in August 1914]</td>\n      <td>[mobilization at perolles in august 1914]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>543</td>\n      <td>JATH_26232_2k_324w.jpg</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7898), tensor(0.7305), tensor(0.6872...</td>\n      <td>[[tensor(97.0861), tensor(137.5495), tensor(14...</td>\n      <td>[soldiers, soldiers, a military vehicle, soldi...</td>\n      <td>950.0</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>Additional service for women, Barracks de la P...</td>\n      <td>additional service for women, barracks de la p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>430</td>\n      <td>JATH_10616_2k_324w.jpg</td>\n      <td>women walking down a street.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.6900), tensor(0.6777), tensor(0.6676...</td>\n      <td>[[tensor(125.4207), tensor(111.6184), tensor(1...</td>\n      <td>[women, women, women, a street, women]</td>\n      <td>836.0</td>\n      <td>women walking down a street.</td>\n      <td>women walking down a street.</td>\n      <td>Procession on the route to the Alps during a w...</td>\n      <td>procession on the route to the alps during a w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>408</td>\n      <td>HAWI_01023_2k_324w.jpg</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7263)]</td>\n      <td>[[tensor(4.5861), tensor(13.1419), tensor(318....</td>\n      <td>[a large tropical cyclone]</td>\n      <td>814.0</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>a a large tropical cyclone.</td>\n      <td>Tornado over Lake Morat</td>\n      <td>tornado over lake morat</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pickle.load(open('../data/dataset_for_segmentation.p','rb'))\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pickle_dir = '../data/phrase_grounding_results/'\n",
    "MDETR_caption = pickle.load(open(pickle_dir + 'MDETR_full_caption.p', 'rb'))\n",
    "MDETR_title = pickle.load(open(pickle_dir + 'MDETR_full_title.p', 'rb'))\n",
    "GLIP_caption = pickle.load(open(pickle_dir + 'GLIP_full_caption.p', 'rb'))\n",
    "GLIP_title = pickle.load(open(pickle_dir + 'GLIP_full_title.p', 'rb'))\n",
    "dataset_dict = pickle.load(open('../data/dataset_for_phrase_grounding/dataset.p', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def GLIP2MDETR(glip_array):\n",
    "    mdetr_array = []\n",
    "    for elem in glip_array:\n",
    "        caption = [elem[1][k -1] if k < len(elem[1]) else elem[1][len(elem[1]) - 1] for k in elem[0].get_field('labels')]\n",
    "        mdetr_array.append([elem[0].get_field('scores'), elem[0].bbox, caption])\n",
    "    return mdetr_array\n",
    "GLIP_caption = GLIP2MDETR(GLIP_caption)\n",
    "GLIP_title = GLIP2MDETR(GLIP_title)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                conf  \\\n0  [tensor(0.8900), tensor(0.9001), tensor(0.9756...   \n1                                   [tensor(0.9990)]   \n2   [tensor(0.9510), tensor(0.9697), tensor(0.9731)]   \n3   [tensor(0.9978), tensor(0.9866), tensor(0.9940)]   \n4  [tensor(0.7897), tensor(0.9859), tensor(0.7141...   \n\n                                                bbox  \\\n0  [[tensor(247.3696), tensor(147.9286), tensor(2...   \n1  [[tensor(81.8896), tensor(160.1014), tensor(22...   \n2  [[tensor(86.9352), tensor(87.6198), tensor(244...   \n3  [[tensor(39.9502), tensor(162.3252), tensor(27...   \n4  [[tensor(56.8723), tensor(86.2393), tensor(273...   \n\n                                                expr  \n0  [ inauguration,  inauguration plaque,  the wal...  \n1                 [ patient of dr xavier cuony city]  \n2                                       [ felsenegg]  \n3  [ his bike,  male ( hermann nussbaumer,  male ...  \n4  [ two women,  regional costume,  regional cost...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conf</th>\n      <th>bbox</th>\n      <th>expr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[tensor(0.8900), tensor(0.9001), tensor(0.9756...</td>\n      <td>[[tensor(247.3696), tensor(147.9286), tensor(2...</td>\n      <td>[ inauguration,  inauguration plaque,  the wal...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[tensor(0.9990)]</td>\n      <td>[[tensor(81.8896), tensor(160.1014), tensor(22...</td>\n      <td>[ patient of dr xavier cuony city]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[tensor(0.9510), tensor(0.9697), tensor(0.9731)]</td>\n      <td>[[tensor(86.9352), tensor(87.6198), tensor(244...</td>\n      <td>[ felsenegg]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[tensor(0.9978), tensor(0.9866), tensor(0.9940)]</td>\n      <td>[[tensor(39.9502), tensor(162.3252), tensor(27...</td>\n      <td>[ his bike,  male ( hermann nussbaumer,  male ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[tensor(0.7897), tensor(0.9859), tensor(0.7141...</td>\n      <td>[[tensor(56.8723), tensor(86.2393), tensor(273...</td>\n      <td>[ two women,  regional costume,  regional cost...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDETR_title = pd.DataFrame(MDETR_title).rename({0:'conf', 1:'bbox', 2:'expr'}, axis=1)\n",
    "MDETR_caption = pd.DataFrame(MDETR_caption).rename({0:'conf', 1:'bbox', 2:'expr'}, axis=1)\n",
    "GLIP_caption = pd.DataFrame(GLIP_caption).rename({0:'conf', 1:'bbox', 2:'expr'}, axis=1)\n",
    "GLIP_title = pd.DataFrame(GLIP_title).rename({0:'conf', 1:'bbox', 2:'expr'}, axis=1)\n",
    "MDETR_title.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_min_bbox(bbox):\n",
    "    try:\n",
    "        return int((bbox[:,2:] - bbox[:,:2]).prod(axis=1).min())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_max_bbox(bbox):\n",
    "    try:\n",
    "        return int((bbox[:,2:] - bbox[:,:2]).prod(axis=1).max())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_median_bbox(bbox):\n",
    "    try:\n",
    "        return int((bbox[:,2:] - bbox[:,:2]).prod(axis=1).median())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "MDETR_title['num_bbox'] = MDETR_title.conf.apply(lambda x: len(x))\n",
    "MDETR_caption['num_bbox'] = MDETR_caption.conf.apply(lambda x: len(x))\n",
    "GLIP_caption['num_bbox'] = GLIP_caption.conf.apply(lambda x: len(x))\n",
    "GLIP_title['num_bbox'] = GLIP_title.conf.apply(lambda x: len(x))\n",
    "\n",
    "MDETR_title['min_bbox_area'] = MDETR_title.bbox.apply(lambda x: get_min_bbox(x))\n",
    "MDETR_caption['min_bbox_area'] = MDETR_caption.bbox.apply(lambda x: get_min_bbox(x))\n",
    "GLIP_caption['min_bbox_area'] = GLIP_caption.bbox.apply(lambda x: get_min_bbox(x))\n",
    "GLIP_title['min_bbox_area'] = GLIP_title.bbox.apply(lambda x: get_min_bbox(x))\n",
    "\n",
    "MDETR_title['max_bbox_area'] = MDETR_title.bbox.apply(lambda x: get_max_bbox(x))\n",
    "MDETR_caption['max_bbox_area'] = MDETR_caption.bbox.apply(lambda x: get_max_bbox(x))\n",
    "GLIP_caption['max_bbox_area'] = GLIP_caption.bbox.apply(lambda x: get_max_bbox(x))\n",
    "GLIP_title['max_bbox_area'] = GLIP_title.bbox.apply(lambda x: get_max_bbox(x))\n",
    "\n",
    "MDETR_title['median_bbox_area'] = MDETR_title.bbox.apply(lambda x: get_median_bbox(x))\n",
    "MDETR_caption['median_bbox_area'] = MDETR_caption.bbox.apply(lambda x: get_median_bbox(x))\n",
    "GLIP_caption['median_bbox_area'] = GLIP_caption.bbox.apply(lambda x: get_median_bbox(x))\n",
    "GLIP_title['median_bbox_area'] = GLIP_title.bbox.apply(lambda x: get_median_bbox(x))\n",
    "\n",
    "\n",
    "MDETR_title.drop(columns=['conf', 'bbox', 'expr'], inplace=True)\n",
    "MDETR_caption.drop(columns=['conf', 'bbox', 'expr'], inplace=True)\n",
    "GLIP_caption.drop(columns=['conf', 'bbox', 'expr'], inplace=True)\n",
    "GLIP_title.drop(columns=['conf', 'bbox', 'expr'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   num_bbox  min_bbox_area  max_bbox_area  median_bbox_area\n0         2         2266.0         2266.0            2266.0\n1         3        42669.0        42669.0           42669.0\n2         1        53524.0        53524.0           53524.0\n3         3        41893.0        67260.0           41893.0\n4         5        33316.0        66190.0           39599.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num_bbox</th>\n      <th>min_bbox_area</th>\n      <th>max_bbox_area</th>\n      <th>median_bbox_area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2266.0</td>\n      <td>2266.0</td>\n      <td>2266.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>42669.0</td>\n      <td>42669.0</td>\n      <td>42669.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>53524.0</td>\n      <td>53524.0</td>\n      <td>53524.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>41893.0</td>\n      <td>67260.0</td>\n      <td>41893.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>33316.0</td>\n      <td>66190.0</td>\n      <td>39599.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLIP_title.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset_dict = pd.DataFrame(dataset_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "dataset_dict['len_caption'] = dataset_dict.caption.apply(lambda x: len(x['raw'].split()) if x['raw'] is not None else 0)\n",
    "dataset_dict['len_title'] = dataset_dict.title.apply(lambda x: len(x['raw'].split()) if x['raw'] is not None else 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   image_id                filename  len_caption  len_title\n0         2  ALCU_00005_2k_324w.jpg            6         18\n1         4  ALCU_00033_2k_324w.jpg            5          8\n2         5  ALNU_00015_2k_324w.jpg            8          5\n3         6  ALNU_00016_2k_324w.jpg            7          6\n4         7  ALNU_00052_2k_324w.jpg            7          6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>filename</th>\n      <th>len_caption</th>\n      <th>len_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>ALCU_00005_2k_324w.jpg</td>\n      <td>6</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>ALCU_00033_2k_324w.jpg</td>\n      <td>5</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>ALNU_00015_2k_324w.jpg</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>ALNU_00016_2k_324w.jpg</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>ALNU_00052_2k_324w.jpg</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = dataset_dict.drop(columns=['title', 'caption'])\n",
    "dataset_dict.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   image_id                filename  len_caption  len_title  \\\n0         2  ALCU_00005_2k_324w.jpg            6         18   \n1         4  ALCU_00033_2k_324w.jpg            5          8   \n2         5  ALNU_00015_2k_324w.jpg            8          5   \n3         6  ALNU_00016_2k_324w.jpg            7          6   \n4         7  ALNU_00052_2k_324w.jpg            7          6   \n\n   num_bbox_MDETR_title  min_bbox_area_MDETR_title  max_bbox_area_MDETR_title  \\\n0                     7                      708.0                    85733.0   \n1                     1                    37941.0                    37941.0   \n2                     3                    52499.0                    54583.0   \n3                     3                    42277.0                    62845.0   \n4                     6                    26689.0                    67220.0   \n\n   median_bbox_area_MDETR_title  num_bbox_MDETR_caption  \\\n0                       76818.0                       4   \n1                       37941.0                       2   \n2                       53046.0                       3   \n3                       44221.0                       2   \n4                       32605.0                       2   \n\n   min_bbox_area_MDETR_caption  max_bbox_area_MDETR_caption  \\\n0                       3684.0                      50244.0   \n1                      39104.0                     146906.0   \n2                      23403.0                      52085.0   \n3                      41603.0                      62280.0   \n4                      68115.0                     113862.0   \n\n   median_bbox_area_MDETR_caption  num_bbox_GLIP_caption  \\\n0                          6862.0                      1   \n1                         39104.0                      1   \n2                         50734.0                      1   \n3                         41603.0                      2   \n4                         68115.0                      3   \n\n   min_bbox_area_GLIP_caption  max_bbox_area_GLIP_caption  \\\n0                     53361.0                     53361.0   \n1                     42747.0                     42747.0   \n2                     52736.0                     52736.0   \n3                     42702.0                     99124.0   \n4                     32610.0                    117776.0   \n\n   median_bbox_area_GLIP_caption  num_bbox_GLIP_title  \\\n0                        53361.0                    2   \n1                        42747.0                    3   \n2                        52736.0                    1   \n3                        42702.0                    3   \n4                        39382.0                    5   \n\n   min_bbox_area_GLIP_title  max_bbox_area_GLIP_title  \\\n0                    2266.0                    2266.0   \n1                   42669.0                   42669.0   \n2                   53524.0                   53524.0   \n3                   41893.0                   67260.0   \n4                   33316.0                   66190.0   \n\n   median_bbox_area_GLIP_title  \n0                       2266.0  \n1                      42669.0  \n2                      53524.0  \n3                      41893.0  \n4                      39599.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>filename</th>\n      <th>len_caption</th>\n      <th>len_title</th>\n      <th>num_bbox_MDETR_title</th>\n      <th>min_bbox_area_MDETR_title</th>\n      <th>max_bbox_area_MDETR_title</th>\n      <th>median_bbox_area_MDETR_title</th>\n      <th>num_bbox_MDETR_caption</th>\n      <th>min_bbox_area_MDETR_caption</th>\n      <th>max_bbox_area_MDETR_caption</th>\n      <th>median_bbox_area_MDETR_caption</th>\n      <th>num_bbox_GLIP_caption</th>\n      <th>min_bbox_area_GLIP_caption</th>\n      <th>max_bbox_area_GLIP_caption</th>\n      <th>median_bbox_area_GLIP_caption</th>\n      <th>num_bbox_GLIP_title</th>\n      <th>min_bbox_area_GLIP_title</th>\n      <th>max_bbox_area_GLIP_title</th>\n      <th>median_bbox_area_GLIP_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>ALCU_00005_2k_324w.jpg</td>\n      <td>6</td>\n      <td>18</td>\n      <td>7</td>\n      <td>708.0</td>\n      <td>85733.0</td>\n      <td>76818.0</td>\n      <td>4</td>\n      <td>3684.0</td>\n      <td>50244.0</td>\n      <td>6862.0</td>\n      <td>1</td>\n      <td>53361.0</td>\n      <td>53361.0</td>\n      <td>53361.0</td>\n      <td>2</td>\n      <td>2266.0</td>\n      <td>2266.0</td>\n      <td>2266.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>ALCU_00033_2k_324w.jpg</td>\n      <td>5</td>\n      <td>8</td>\n      <td>1</td>\n      <td>37941.0</td>\n      <td>37941.0</td>\n      <td>37941.0</td>\n      <td>2</td>\n      <td>39104.0</td>\n      <td>146906.0</td>\n      <td>39104.0</td>\n      <td>1</td>\n      <td>42747.0</td>\n      <td>42747.0</td>\n      <td>42747.0</td>\n      <td>3</td>\n      <td>42669.0</td>\n      <td>42669.0</td>\n      <td>42669.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>ALNU_00015_2k_324w.jpg</td>\n      <td>8</td>\n      <td>5</td>\n      <td>3</td>\n      <td>52499.0</td>\n      <td>54583.0</td>\n      <td>53046.0</td>\n      <td>3</td>\n      <td>23403.0</td>\n      <td>52085.0</td>\n      <td>50734.0</td>\n      <td>1</td>\n      <td>52736.0</td>\n      <td>52736.0</td>\n      <td>52736.0</td>\n      <td>1</td>\n      <td>53524.0</td>\n      <td>53524.0</td>\n      <td>53524.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>ALNU_00016_2k_324w.jpg</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>42277.0</td>\n      <td>62845.0</td>\n      <td>44221.0</td>\n      <td>2</td>\n      <td>41603.0</td>\n      <td>62280.0</td>\n      <td>41603.0</td>\n      <td>2</td>\n      <td>42702.0</td>\n      <td>99124.0</td>\n      <td>42702.0</td>\n      <td>3</td>\n      <td>41893.0</td>\n      <td>67260.0</td>\n      <td>41893.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>ALNU_00052_2k_324w.jpg</td>\n      <td>7</td>\n      <td>6</td>\n      <td>6</td>\n      <td>26689.0</td>\n      <td>67220.0</td>\n      <td>32605.0</td>\n      <td>2</td>\n      <td>68115.0</td>\n      <td>113862.0</td>\n      <td>68115.0</td>\n      <td>3</td>\n      <td>32610.0</td>\n      <td>117776.0</td>\n      <td>39382.0</td>\n      <td>5</td>\n      <td>33316.0</td>\n      <td>66190.0</td>\n      <td>39599.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = pd.merge(dataset_dict, MDETR_title, left_index=True, right_index=True).rename({'num_bbox': 'num_bbox_MDETR_title', 'min_bbox_area': 'min_bbox_area_MDETR_title', 'max_bbox_area': 'max_bbox_area_MDETR_title', 'median_bbox_area': 'median_bbox_area_MDETR_title'}, axis=1)\n",
    "dataset_dict = pd.merge(dataset_dict, MDETR_caption, left_index=True, right_index=True).rename({'num_bbox': 'num_bbox_MDETR_caption', 'min_bbox_area': 'min_bbox_area_MDETR_caption', 'max_bbox_area': 'max_bbox_area_MDETR_caption', 'median_bbox_area': 'median_bbox_area_MDETR_caption'}, axis=1)\n",
    "dataset_dict = pd.merge(dataset_dict, GLIP_caption, left_index=True, right_index=True).rename({'num_bbox': 'num_bbox_GLIP_caption', 'min_bbox_area': 'min_bbox_area_GLIP_caption', 'max_bbox_area': 'max_bbox_area_GLIP_caption', 'median_bbox_area': 'median_bbox_area_GLIP_caption'}, axis=1)\n",
    "dataset_dict = pd.merge(dataset_dict, GLIP_title, left_index=True, right_index=True).rename({'num_bbox': 'num_bbox_GLIP_title', 'min_bbox_area': 'min_bbox_area_GLIP_title', 'max_bbox_area': 'max_bbox_area_GLIP_title', 'median_bbox_area': 'median_bbox_area_GLIP_title'}, axis=1)\n",
    "dataset_dict.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "    idx                filename  \\\n0  1027  JOMU_32980_2k_324w.jpg   \n1   183  CAPO_02480_2k_324w.jpg   \n2   543  JATH_26232_2k_324w.jpg   \n3   430  JATH_10616_2k_324w.jpg   \n4   408  HAWI_01023_2k_324w.jpg   \n\n                                             caption model     expr  \\\n0                people buying sweets at the market.  GLIP  caption   \n1  a group of soldiers stand in front of a building.  GLIP  caption   \n2     soldiers stand in front of a military vehicle.  GLIP  caption   \n3                       women walking down a street.  GLIP  caption   \n4          a photograph of a large tropical cyclone.  GLIP  caption   \n\n                                                conf  \\\n0  [tensor(0.8280), tensor(0.7924), tensor(0.6899...   \n1  [tensor(0.8242), tensor(0.5884), tensor(0.5781...   \n2  [tensor(0.7898), tensor(0.7305), tensor(0.6872...   \n3  [tensor(0.6900), tensor(0.6777), tensor(0.6676...   \n4                                   [tensor(0.7263)]   \n\n                                                bbox  \\\n0  [[tensor(178.8594), tensor(116.5122), tensor(2...   \n1  [[tensor(0.8775), tensor(131.9302), tensor(81....   \n2  [[tensor(97.0861), tensor(137.5495), tensor(14...   \n3  [[tensor(125.4207), tensor(111.6184), tensor(1...   \n4  [[tensor(4.5861), tensor(13.1419), tensor(318....   \n\n                                              labels  image_id  \\\n0  [people, people, people, people, people, peopl...    1488.0   \n1     [soldiers, a building, a building, a building]     516.0   \n2  [soldiers, soldiers, a military vehicle, soldi...     950.0   \n3             [women, women, women, a street, women]     836.0   \n4                         [a large tropical cyclone]     814.0   \n\n                                         caption_raw  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4          a photograph of a large tropical cyclone.   \n\n                                caption_preprocessed  \\\n0                people buying sweets at the market.   \n1  a group of soldiers stand in front of a building.   \n2     soldiers stand in front of a military vehicle.   \n3                       women walking down a street.   \n4                        a a large tropical cyclone.   \n\n                                           title_raw  \\\n0  Saint Nicholas Festival Market, Place de Notre...   \n1          [Mobilization at Perolles in August 1914]   \n2  Additional service for women, Barracks de la P...   \n3  Procession on the route to the Alps during a w...   \n4                            Tornado over Lake Morat   \n\n                                  title_preprocessed  \n0  saint nicholas festival market, place de notre...  \n1          [mobilization at perolles in august 1914]  \n2  additional service for women, barracks de la p...  \n3  procession on the route to the alps during a w...  \n4                            tornado over lake morat  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>filename</th>\n      <th>caption</th>\n      <th>model</th>\n      <th>expr</th>\n      <th>conf</th>\n      <th>bbox</th>\n      <th>labels</th>\n      <th>image_id</th>\n      <th>caption_raw</th>\n      <th>caption_preprocessed</th>\n      <th>title_raw</th>\n      <th>title_preprocessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1027</td>\n      <td>JOMU_32980_2k_324w.jpg</td>\n      <td>people buying sweets at the market.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8280), tensor(0.7924), tensor(0.6899...</td>\n      <td>[[tensor(178.8594), tensor(116.5122), tensor(2...</td>\n      <td>[people, people, people, people, people, peopl...</td>\n      <td>1488.0</td>\n      <td>people buying sweets at the market.</td>\n      <td>people buying sweets at the market.</td>\n      <td>Saint Nicholas Festival Market, Place de Notre...</td>\n      <td>saint nicholas festival market, place de notre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>183</td>\n      <td>CAPO_02480_2k_324w.jpg</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.8242), tensor(0.5884), tensor(0.5781...</td>\n      <td>[[tensor(0.8775), tensor(131.9302), tensor(81....</td>\n      <td>[soldiers, a building, a building, a building]</td>\n      <td>516.0</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>a group of soldiers stand in front of a building.</td>\n      <td>[Mobilization at Perolles in August 1914]</td>\n      <td>[mobilization at perolles in august 1914]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>543</td>\n      <td>JATH_26232_2k_324w.jpg</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7898), tensor(0.7305), tensor(0.6872...</td>\n      <td>[[tensor(97.0861), tensor(137.5495), tensor(14...</td>\n      <td>[soldiers, soldiers, a military vehicle, soldi...</td>\n      <td>950.0</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>soldiers stand in front of a military vehicle.</td>\n      <td>Additional service for women, Barracks de la P...</td>\n      <td>additional service for women, barracks de la p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>430</td>\n      <td>JATH_10616_2k_324w.jpg</td>\n      <td>women walking down a street.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.6900), tensor(0.6777), tensor(0.6676...</td>\n      <td>[[tensor(125.4207), tensor(111.6184), tensor(1...</td>\n      <td>[women, women, women, a street, women]</td>\n      <td>836.0</td>\n      <td>women walking down a street.</td>\n      <td>women walking down a street.</td>\n      <td>Procession on the route to the Alps during a w...</td>\n      <td>procession on the route to the alps during a w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>408</td>\n      <td>HAWI_01023_2k_324w.jpg</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>GLIP</td>\n      <td>caption</td>\n      <td>[tensor(0.7263)]</td>\n      <td>[[tensor(4.5861), tensor(13.1419), tensor(318....</td>\n      <td>[a large tropical cyclone]</td>\n      <td>814.0</td>\n      <td>a photograph of a large tropical cyclone.</td>\n      <td>a a large tropical cyclone.</td>\n      <td>Tornado over Lake Morat</td>\n      <td>tornado over lake morat</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "merged_dataset = pd.merge(dataset, dataset_dict.drop('image_id', axis=1), left_on='filename', right_on='filename', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_choice(x):\n",
    "    if x['model'] == 'GLIP' and x['expr'] == 'caption':\n",
    "        return np.array([1,0,0,0])\n",
    "    elif x['model'] == 'GLIP' and x['expr'] == 'title':\n",
    "        return np.array([0,1,0,0])\n",
    "    elif x['model'] == 'MDETR' and x['expr'] == 'caption':\n",
    "        return np.array([0,0,1,0])\n",
    "    elif x['model'] == 'MDETR' and x['expr'] == 'title':\n",
    "        return np.array([0,0,0,1])\n",
    "merged_dataset['choice'] = merged_dataset.apply(lambda x: get_choice(x), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "GLIP     1354\nMDETR     106\nName: model, dtype: int64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.model.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "caption    1221\ntitle       239\nName: expr, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.expr.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "merged_dataset['isGLIP'] = merged_dataset.model.apply(lambda x: x=='GLIP')\n",
    "merged_dataset['isCaption'] = merged_dataset.expr.apply(lambda x: x=='caption')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "1130"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((merged_dataset.model == 'GLIP') & (merged_dataset.expr==('caption'))).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "224"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((merged_dataset.model == 'GLIP') & (merged_dataset.expr==('title'))).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "91"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((merged_dataset.model == 'MDETR') & (merged_dataset.expr==('caption'))).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((merged_dataset.model == 'MDETR') & (merged_dataset.expr==('title'))).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "|         | MDETR | GLIP |\n",
    "|---------|-------|------|\n",
    "| Caption | 91    | 1130 |\n",
    "| Title   | 15    | 224  |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7251/1766659751.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  merged_dataset['conf'] =  merged_dataset.conf.apply(lambda x: torch.tensor(x).numpy())\n",
      "/tmp/ipykernel_7251/1766659751.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  merged_dataset['bbox'] =  merged_dataset.bbox.apply(lambda x: torch.tensor(x).numpy())\n"
     ]
    }
   ],
   "source": [
    "merged_dataset['conf'] =  merged_dataset.conf.apply(lambda x: torch.tensor(x).numpy())\n",
    "merged_dataset['bbox'] =  merged_dataset.bbox.apply(lambda x: torch.tensor(x).numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "labels = np.stack(np.array(merged_dataset['choice']))\n",
    "features= merged_dataset.drop(['choice', 'idx', 'filename', 'caption', 'caption_raw', 'caption_preprocessed', 'title_raw', 'title_preprocessed', 'model', 'expr', 'isGLIP', 'isCaption', 'labels', 'conf', 'bbox'], axis = 1).fillna(0)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.4880e+03, 6.0000e+00, 9.0000e+00, ..., 1.0236e+05, 1.0236e+05,\n        1.0236e+05],\n       [5.1600e+02, 1.0000e+01, 6.0000e+00, ..., 4.0021e+04, 4.0021e+04,\n        4.0021e+04],\n       [9.5000e+02, 8.0000e+00, 9.0000e+00, ..., 2.7530e+03, 5.5180e+03,\n        2.7530e+03],\n       ...,\n       [7.0500e+02, 5.0000e+00, 7.0000e+00, ..., 2.6954e+04, 6.2835e+04,\n        2.7314e+04],\n       [7.3600e+02, 5.0000e+00, 1.0000e+01, ..., 1.0009e+04, 2.4082e+04,\n        1.0966e+04],\n       [1.9600e+02, 7.0000e+00, 1.2000e+01, ..., 0.0000e+00, 0.0000e+00,\n        0.0000e+00]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1009, 19)\n",
      "Training Labels Shape: (1009, 4)\n",
      "Testing Features Shape: (337, 19)\n",
      "Testing Labels Shape: (337, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average baseline error:  0.12\n"
     ]
    }
   ],
   "source": [
    "# The baseline predictions are the historical averages\n",
    "baseline_preds = np.array([1,0,0,0])\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - test_labels)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 22:00:57.459637: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleClassifierCompact(nn.Module):\n",
    "    num_hidden : int   # Number of hidden neurons\n",
    "    num_outputs : int  # Number of output neurons\n",
    "\n",
    "    @nn.compact  # Tells Flax to look for defined submodules\n",
    "    def __call__(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        # while defining necessary layers\n",
    "        x = nn.relu(nn.Dense(features=self.num_hidden)(x))\n",
    "        x = nn.relu(nn.Dense(features=self.num_hidden)(x))\n",
    "        x = nn.relu(nn.Dense(features=self.num_hidden)(x))\n",
    "        x = nn.Dense(features=self.num_outputs)(x)\n",
    "        return x\n",
    "\n",
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return jnp.inner(y-pred, y-pred) / 2.0\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "\n",
    "\n",
    "model = SimpleClassifierCompact(num_hidden=8, num_outputs=4)\n",
    "batch = jnp.ones((8, 19))\n",
    "params = model.init(jax.random.PRNGKey(0), batch)\n",
    "\n",
    "class PyDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, features, label, seed=42):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            seed - The seed to use to create the PRNG state with which we want to generate the data points\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.np_rng = np.random.RandomState(seed=seed)\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.features[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label\n",
    "\n",
    "train_dataset = PyDataset(train_features, train_labels)\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "tx = optax.sgd(learning_rate=0.001)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                            params=params,\n",
    "                                            tx=tx)\n",
    "\n",
    "\n",
    "def calculate_loss_acc(state, params, batch):\n",
    "    data_input, labels = batch\n",
    "    # Obtain the logits and predictions of the model for the input data\n",
    "    logits = state.apply_fn(params, data_input)\n",
    "    pred_labels = (logits > 0).astype(jnp.float32)\n",
    "    # Calculate the loss and accuracy\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "    acc = (pred_labels == labels).mean()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss\n",
    "                                 argnums=1,  # Parameters are second argument of the function\n",
    "                                 has_aux=True  # Function has additional outputs, here accuracy\n",
    "                                )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, acc), grads = grad_fn(state, state.params, batch)\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss, acc\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc\n",
    "\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=numpy_collate)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(state, data_loader, num_epochs=100):\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for batch in data_loader:\n",
    "            state, loss, acc = train_step(state, batch)\n",
    "            # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "            # For simplicity, we skip this part here\n",
    "    return state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [00:06<00:00, 83.31it/s]\n"
     ]
    }
   ],
   "source": [
    "trained_model_state = train_model(model_state, train_data_loader, num_epochs=500)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "test_dataset = PyDataset(test_features, test_labels)\n",
    "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
    "test_data_loader = data.DataLoader(test_dataset,\n",
    "                                   batch_size=128,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=False,\n",
    "                                   collate_fn=numpy_collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def eval_model(state, data_loader):\n",
    "    all_accs, batch_sizes = [], []\n",
    "    for batch in data_loader:\n",
    "        batch_acc = eval_step(state, batch)\n",
    "        all_accs.append(batch_acc)\n",
    "        batch_sizes.append(batch[0].shape[0])\n",
    "    # Weighted average since some batches might be smaller\n",
    "    acc = sum([a*b for a,b in zip(all_accs, batch_sizes)]) / sum(batch_sizes)\n",
    "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 90.27%\n"
     ]
    }
   ],
   "source": [
    "eval_model(trained_model_state, test_data_loader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0.773972602739726"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((merged_dataset.model == 'GLIP') & (merged_dataset.expr==('caption'))).sum() / len(merged_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
